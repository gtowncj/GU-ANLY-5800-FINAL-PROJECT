# -*- coding: utf-8 -*-
"""DSAN_5800_Final_Project_Evaluation_Notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UZmLV8ZwfQvduAn4Aha9AuyumNvdMiKG

# Stand Alone Notebook to Evaluate Fine Tuned Gemma Models

### Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# Install Pytorch & other libraries
# %pip install "torch>=2.4.0" tensorboard

# Install Gemma release branch from Hugging Face
# %pip install "transformers>=4.51.3"

# Install Hugging Face libraries
# %pip install  --upgrade \


## REQUIREMENTS

#   "datasets>=3.3.2" \
#   "accelerate==1.4.0" \
#   "evaluate==0.4.3" \
#   "bitsandbytes==0.45.3" \
#   "trl==0.21.0" \
#   "peft==0.14.0" \
#   "protobuf==5.29.1" \
#   "fsspec==2025.3.0" \
#   python-Levenshtein \
#   sentencepiece

import os
os.environ["HF_HUB_DISABLE_XET"] = "1"

import jax
print("JAX backend:", jax.default_backend())
print("JAX devices:", jax.devices())

from google.colab import drive
from google.colab import userdata
from huggingface_hub import login

import matplotlib.pyplot as plt
import seaborn as sns

import os
from pathlib import Path
import json
import random
import re
import sqlite3

import torch
from datasets import load_dataset
import pandas as pd
import Levenshtein

from huggingface_hub import login
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForImageTextToText,
    BitsAndBytesConfig,
)
from peft import LoraConfig, PeftModel
from trl import SFTConfig, SFTTrainer

# Login into Hugging Face Hub
hf_token = userdata.get('HF_TOKEN') # If you are running inside a Google Colab
login(hf_token)

"""### Setup Repo Structure"""

drive.mount('/content/drive')

ROOT_DRIVE_DIR = "/content/drive/MyDrive/gemma_lora_ft"

WIKISQL_DIR = Path("/content/WikiSQL")

DATA_DIR = Path("/content/data")

TRAINED_MODEL_NAME = "gemma_text_to_sql_run_train_rank_32_20251208_075911"

TRAINED_MODEL_DIR = f"{ROOT_DRIVE_DIR}/{TRAINED_MODEL_NAME}"

"""### System/User Prompt with Prompt Builder"""

# User prompt template (used in both training & inference)

def generate_raw_prompt(question: str, schema_text: str) -> str:
    return f"""<INSTRUCTIONS>
You are a precise text-to-SQL generator. Using the known schema of the sql database you must output only a valid SQL query and nothing else.
</INSTRUCTIONS>

<SCHEMA>
{schema_text}
</SCHEMA>

<QUESTION>
{question}
</QUESTION>

<SQL_Query>
""".strip()

"""# Load Test WikiSQL Dataset"""

if not os.path.exists(WIKISQL_DIR):
  !git clone https://github.com/salesforce/WikiSQL

if not os.path.exists(DATA_DIR):
  !tar xvjf /content/WikiSQL/data.tar.bz2

"""### SQLite Setup and Table Builder"""

def load_tables(split="dev"):
    tables_path = DATA_DIR / f"{split}.tables.jsonl"
    tables = {}
    with open(tables_path, "r", encoding="utf-8") as f:
        for line in f:
            obj = json.loads(line)
            tables[obj["id"]] = obj
    return tables


def iter_split(split="dev"):
    q_path = DATA_DIR / f"{split}.jsonl"
    tables = load_tables(split)
    with open(q_path, "r", encoding="utf-8") as f:
        for line in f:
            ex = json.loads(line)
            table = tables[ex["table_id"]]
            yield ex, table


def build_sqlite_from_table(table_obj):
    df = pd.DataFrame(table_obj["rows"], columns=table_obj["header"])
    conn = sqlite3.connect(":memory:")
    df.to_sql("data", conn, index=False, if_exists="replace")
    return conn


AGG_OPS = ["", "MAX", "MIN", "COUNT", "SUM", "AVG"]
COND_OPS = ["=", ">", "<", "OP"]


def escape_identifier(name: str) -> str:
    # Escape internal quotes by doubling them
    cleaned = name.replace('"', '""')
    return f'"{cleaned}"'


def logical_to_sql(sql_obj, table_obj, table_name="data"):
    sel_idx = sql_obj["sel"]
    agg_idx = sql_obj["agg"]
    conds = sql_obj["conds"]

    columns = table_obj["header"]
    sel_col = escape_identifier(columns[sel_idx])
    agg = AGG_OPS[agg_idx]

    # FIX: Proper aggregation syntax
    if agg == "":
        select_expr = sel_col
    else:
        select_expr = f"{agg}({sel_col})"

    query = f"SELECT {select_expr} FROM {table_name}"

    where_clauses = []
    for col_idx, op_idx, val in conds:
        col_name = escape_identifier(columns[col_idx])
        op = COND_OPS[op_idx]

        if isinstance(val, str):
            v_str = "'" + val.replace("'", "''") + "'"
        else:
            v_str = str(val)

        if op == "OP":
            op = "="

        where_clauses.append(f"{col_name} {op} {v_str}")

    if where_clauses:
        query += " WHERE " + " AND ".join(where_clauses)

    return query

def execute_sql(conn, sql):
    try:
        cur = conn.cursor()
        cur.execute(sql)
        return cur.fetchall(), None
    except Exception as e:
        return None, str(e)


def make_schema_text(table_obj):
    """
    Convert WikiSQL table to the same schema format used for synthetic data,
    so the model sees consistent inputs.
    """
    cols = table_obj["header"]
    return "\n".join(f"- {c} (TEXT)" for c in cols)

"""### Test SQL Helper Functions"""

import sqlite3
import re

def is_sql_syntax_valid(sql):
    """
    Checks SQL syntax by replacing table names with a dummy table
    so that missing tables do NOT cause an error.
    """
    sql_clean = sql.strip()

    # Replace any token after FROM or JOIN with the dummy table name
    sql_clean = re.sub(r"(FROM|JOIN)\s+[\w\.\-]+", r"\1 dummy", sql_clean, flags=re.IGNORECASE)

    conn = sqlite3.connect(":memory:")
    conn.execute("CREATE TABLE dummy(x TEXT);")

    try:
        conn.execute(sql_clean)
        return True
    except Exception as e:
        if "no such table" in str(e).lower():
            return True   # table does not exist â†’ ignore
        return False

wikisql_valid = 0
wikisql_total = 0
wikisql_errors = []

for ex, tbl in iter_split("test"):
    wikisql_total += 1

    conn = build_sqlite_from_table(tbl)
    sql = logical_to_sql(ex["sql"], tbl)

    result, err = execute_sql(conn, sql)

    if err is None:
        wikisql_valid += 1
    else:
        wikisql_errors.append((sql, err))

print(f"WikiSQL valid queries: {wikisql_valid}/{wikisql_total} "
      f"({wikisql_valid / wikisql_total:.2%})")

print("\nExample WikiSQL errors:")
if not wikisql_errors:
    print("No errors â€” all WikiSQL queries are valid!")
else:
    for i, (sql, err) in enumerate(wikisql_errors[:5]):
        print("SQL:", sql)
        print("ERROR:", err, "\n")

"""### Final Data Preprocessing"""

from datasets import Dataset

def normalize_sql(sql: str) -> str:
    sql = sql.strip().rstrip(";")
    sql = sql.replace("`", '"')
    return sql

def build_completion(sql: str) -> str:
    sql = normalize_sql(sql)
    return sql.lstrip() + "\n</SQL_Query>"

def build_training_row_from_wikisql(ex, tbl):
    schema = make_schema_text(tbl)
    prompt = generate_raw_prompt(ex["question"], schema)
    gold_sql = logical_to_sql(ex["sql"], tbl)
    completion = build_completion(gold_sql)

    return {
        "prompt": prompt,
        "completion": completion
    }

"""### Build Datasets"""

train_rows = []
for ex, tbl in iter_split("train"):
    train_rows.append(build_training_row_from_wikisql(ex, tbl))

test_rows = []
for ex, tbl in iter_split("test"):
    test_rows.append(build_training_row_from_wikisql(ex, tbl))

val_rows = []
for ex, tbl in iter_split("dev"):
    val_rows.append(build_training_row_from_wikisql(ex, tbl))

train_wikisql = Dataset.from_list(train_rows)
test_wikisql = Dataset.from_list(test_rows)
val_wikisql = Dataset.from_list(val_rows)

print(train_wikisql)
print(test_wikisql)
print(val_wikisql)

# print("\n")

# for i in range(3):
#     print("PROMPT:\n", train_wikisql[i]["prompt"])
#     print("COMPLETION:\n", train_wikisql[i]["completion"])
#     print("="*80)

"""# Training Visualizations

### Load Training Log
"""

# Paths
TRAINING_LOG_PATH = f"{TRAINED_MODEL_DIR}/training_log.json"

# Load JSON list of dicts
with open(TRAINING_LOG_PATH, "r") as f:
    log_history = json.load(f)

print("Loaded", len(log_history), "log entries.")

"""### Plotting Functions"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import os


# ===========================
#  PLOTTING UTIL
# ===========================

def _save_or_show(save_path):
    """Helper: save figure if path is provided, else show."""
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"Saved plot to: {save_path}")
    else:
        plt.show()


# ===========================
#   LOSS CURVE + EMA
# ===========================

def plot_loss(df: pd.DataFrame, span: int = 50, save_path: str = None):
    """
    Plot raw loss + EMA-smoothed loss.
    """
    if "loss" not in df.columns:
        print("No `loss` column found in dataframe.")
        return

    df = df.copy()
    df["loss_ema"] = df["loss"].ewm(span=span).mean()

    plt.figure(figsize=(10, 6))
    sns.lineplot(data=df, x="step", y="loss", alpha=0.4, label="Raw Loss")
    sns.lineplot(data=df, x="step", y="loss_ema", label=f"EMA Loss (span={span})")

    plt.title("Training Loss (Raw vs EMA Smoothed)")
    plt.xlabel("Training Step")
    plt.ylabel("Loss")
    plt.grid(True)
    plt.legend()

    _save_or_show(save_path)


# ===========================
#   GRADIENT NORM
# ===========================

def plot_grad_norm(df: pd.DataFrame, save_path: str = None):
    """
    Plot gradient norm over training.
    """
    if "grad_norm" not in df.columns:
        print("No `grad_norm` column found in dataframe.")
        return

    grad_df = df.dropna(subset=["grad_norm"])

    plt.figure(figsize=(10, 6))
    sns.lineplot(data=grad_df, x="step", y="grad_norm")

    plt.title("Gradient Norm Over Training")
    plt.xlabel("Training Step")
    plt.ylabel("Grad Norm")
    plt.grid(True)

    _save_or_show(save_path)


# ===========================
#   TOKEN ACCURACY
# ===========================

def plot_token_accuracy(df: pd.DataFrame, save_path: str = None):
    """
    Plot mean token accuracy over training.
    """
    if "mean_token_accuracy" not in df.columns:
        print("No `mean_token_accuracy` column found in dataframe.")
        return

    acc_df = df.dropna(subset=["mean_token_accuracy"])

    plt.figure(figsize=(10, 6))
    sns.lineplot(data=acc_df, x="step", y="mean_token_accuracy")

    plt.title("Mean Token Accuracy Over Time")
    plt.xlabel("Training Step")
    plt.ylabel("Mean Token Accuracy")
    plt.grid(True)

    _save_or_show(save_path)


# ===========================
#   LEARNING RATE
# ===========================

def plot_learning_rate(df: pd.DataFrame, save_path: str = None):
    """
    Plot learning rate schedule.
    """
    if "learning_rate" not in df.columns:
        print("No `learning_rate` column found in dataframe.")
        return

    plt.figure(figsize=(10, 6))
    sns.lineplot(data=df, x="step", y="learning_rate")

    plt.title("Learning Rate Over Training")
    plt.xlabel("Training Step")
    plt.ylabel("Learning Rate")
    plt.grid(True)

    _save_or_show(save_path)

training_metrics_df = pd.DataFrame(log_history)

plot_loss(training_metrics_df)
plot_grad_norm(training_metrics_df)
plot_token_accuracy(training_metrics_df)

"""# Evaluation Functions

### Generate Cleaned SQL Query from Gemma Model with Prompt
"""

import re

def clean_sql_output(text: str) -> str:
    """
    Extract a clean SQL statement from LLM output under the new tag-based format:

        <SQL_Query>
        SELECT ...
        </SQL_Query>

    Handles:
      - missing or extra whitespace
      - missing closing tags
      - trailing commentary after </SQL_Query>
      - selects the FIRST valid SQL statement
    """

    if not text or not isinstance(text, str):
        return ""

    raw = text.strip()

    # -------------------------------
    # 1. Extract content inside <SQL_Query> ... </SQL_Query>
    # -------------------------------
    m = re.search(
        r"<SQL_Query>(.*?)(</SQL_Query>|$)",
        raw,
        flags=re.IGNORECASE | re.DOTALL,
    )

    if m:
        candidate = m.group(1).strip()
    else:
        # fallback if tag missing
        candidate = raw

    # Remove any accidental tag echoes
    candidate = re.sub(r"</?SQL_Query>", "", candidate, flags=re.IGNORECASE).strip()

    # Strip markdown fences if any
    candidate = re.sub(r"```sql", "", candidate, flags=re.IGNORECASE)
    candidate = candidate.replace("```", "").strip()

    # Normalize whitespace
    candidate = re.sub(r"[ \t]+", " ", candidate)

    # -------------------------------
    # 2. Grab the first SQL keyword (fallback)
    # -------------------------------
    sql_start = re.compile(
        r"\b(SELECT|INSERT\s+INTO|UPDATE|DELETE\s+FROM|CREATE\s+TABLE)\b",
        flags=re.IGNORECASE,
    )

    match = sql_start.search(candidate)
    if not match:
        return candidate  # return raw candidate (probably empty)

    sql = candidate[match.start():].strip()

    # -------------------------------
    # 3. Remove trailing commentary or extra content
    # -------------------------------
    stop_tokens = [
        "</SQL_Query>",
        "<INSTRUCTIONS>",
        "<QUESTION>",
        "<SCHEMA>",
        "Explanation:",
        "Answer:",
        "Result:",
        "Note:",
        "\n#",
        "```",
    ]

    end_positions = []
    for tok in stop_tokens:
        pos = sql.find(tok)
        if pos > 0:
            end_positions.append(pos)

    if end_positions:
        sql = sql[:min(end_positions)].strip()

    # Remove trailing punctuation
    sql = sql.rstrip(";` ")

    return sql.strip()



def generate_sql_from_llm(question: str, schema_text: str, model) -> str:
    """
    Generate SQL from a raw text prompt using continuation-style generation.
    """
    # Build the same raw prompt used during training
    prompt = generate_raw_prompt(question, schema_text)

    # print(prompt)

    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    print
    # Compute input length to slice off the prompt from model output
    input_len = inputs["input_ids"].shape[-1]

    # Generate continuation
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            do_sample=False,  # deterministic for evaluation
            pad_token_id=tokenizer.eos_token_id
        )

    # Slice off the prompt to isolate model-generated SQL
    gen_tokens = outputs[0][input_len:]
    decoded = tokenizer.decode(gen_tokens, skip_special_tokens=True)

    return clean_sql_output(decoded)


# SQL similarity score evaluator
def tokenize_sql(sql: str):
    sql = sql.lower()
    sql = re.sub(r"[^a-z0-9_*]", " ", sql)
    tokens = sql.split()
    return tokens

"""### Optional Case Insensitivity Function"""

def apply_case_insensitive_matching(sql: str) -> str:
    """
    Rewrites any equality of the form:
        "Column" = 'Value'
    into:
        "Column" = 'Value' COLLATE NOCASE
    Only applies to string literal comparisons.
    """

    # pattern matches: "Column" = 'Value'
    pattern = r'(".*?")\s*=\s*(\'[^\']*\')'

    def repl(match):
        col, val = match.groups()
        return f'{col} = {val} COLLATE NOCASE'

    return re.sub(pattern, repl, sql)

"""### Semantic Similarity Functions"""

def jaccard_similarity(sql1: str, sql2: str) -> float:
    """
    Jaccard similarity over token sets.
    Returns value in [0,1].
    """
    t1 = set(tokenize_sql(sql1))
    t2 = set(tokenize_sql(sql2))

    if not t1 and not t2:
        return 1.0
    return len(t1 & t2) / len(t1 | t2)


def levenshtein_similarity(sql1: str, sql2: str) -> float:
    """
    Normalized Levenshtein similarity.
    1.0 = identical strings.
    """
    sql1 = sql1.lower().strip()
    sql2 = sql2.lower().strip()

    if not sql1 and not sql2:
        return 1.0

    dist = Levenshtein.distance(sql1, sql2)
    max_len = max(len(sql1), len(sql2))

    if max_len == 0:
        return 1.0
    return 1 - (dist / max_len)

def extract_select_columns(sql: str):
    """
    Extract SELECT columns robustly.
    Handles cases like:
       SELECT a, b FROM ...
       SELECT COUNT(a) FROM ...
       SELECT MAX("col name") FROM ...
    """
    sql = sql.lower().strip()
    m = re.search(r"select\s+(.*?)\s+from", sql)
    if not m:
        return set()

    cols = m.group(1)

    # Remove function wrappers like count(), max(), avg()
    cols = re.sub(r"\b(count|max|min|sum|avg)\s*\(", "", cols)
    cols = cols.replace(")", "")

    # Split multiple columns
    parts = [c.strip(' "\'') for c in cols.split(",")]
    return set(p for p in parts if p)


def extract_where_columns(sql: str):
    """
    Extract WHERE clause column names.
    Handles AND, multiple conditions, operators, quoted columns.
    """
    sql = sql.lower().strip()
    m = re.search(r"where\s+(.*)", sql)
    if not m:
        return set()

    conditions = m.group(1)

    # Split by AND
    conds = re.split(r"\band\b", conditions)

    cols = []
    for cond in conds:
        parts = cond.strip().split()
        if parts:
            col = parts[0].strip('"\'')
            cols.append(col)

    return set(cols)

def extract_sql_operators(sql: str):
    """
    Extract operators used in SQL conditions.
    """
    ops = set()
    for op in ["=", ">", "<", "!=", "<=", ">=", "like"]:
        if op in sql.lower():
            ops.add(op)
    return ops


def structural_similarity(sql1: str, sql2: str):
    """
    Returns a dict with:
       - select_match
       - where_match
       - op_match
    Each value is in [0,1].
    """

    s1_select = extract_select_columns(sql1)
    s2_select = extract_select_columns(sql2)

    s1_where = extract_where_columns(sql1)
    s2_where = extract_where_columns(sql2)

    s1_ops = extract_sql_operators(sql1)
    s2_ops = extract_sql_operators(sql2)

    def overlap(a, b):
        if not a and not b:
            return 1.0
        return len(a & b) / max(1, len(a | b))

    return {
        "select_match": overlap(s1_select, s2_select),
        "where_match": overlap(s1_where, s2_where),
        "op_match": overlap(s1_ops, s2_ops),
    }

"""### Experiment Evaluation Function"""

import random
import json
import time
from tqdm import tqdm
import pandas as pd

def evaluate_model(
    model,
    sample_fraction=0.10,
    case_insensitive_sql=True,
    log_path=None,
    verbose=False,
    seed=42
):
    """
    Runs a full evaluation on a fraction of the WikiSQL test set.
    Returns a DataFrame with one row per example.
    """

    random.seed(seed)

    examples = list(iter_split("test"))
    total = len(examples)
    sample_size = int(total * sample_fraction)

    sampled = random.sample(examples, sample_size)

    rows = []
    start_time = time.time()

    for idx, (ex, tbl) in enumerate(tqdm(sampled, desc="Evaluating")):
        conn = build_sqlite_from_table(tbl)
        schema = make_schema_text(tbl)

        # Gold SQL
        gold_sql = logical_to_sql(ex["sql"], tbl)
        gold_res, gold_err = execute_sql(conn, gold_sql)

        # LLM prediction
        pred_sql = generate_sql_from_llm(ex["question"], schema, model)

        # Apply case-insensitive matching for eval execution only
        pred_sql_exec = apply_case_insensitive_matching(pred_sql) if case_insensitive_sql else pred_sql
        pred_res, pred_err = execute_sql(conn, pred_sql_exec)

        # Semantic comparisons (no normalization)
        jac = jaccard_similarity(gold_sql, pred_sql)
        lev = levenshtein_similarity(gold_sql, pred_sql)
        struct = structural_similarity(gold_sql, pred_sql)

        # Classification
        exec_valid = (pred_err is None)
        exec_correct = (exec_valid and gold_err is None and pred_res == gold_res)

        rows.append({
            "question": ex["question"],
            "schema": schema,
            "gold_sql": gold_sql,
            "pred_sql": pred_sql,
            "pred_sql_exec": pred_sql_exec,
            "gold_res": gold_res,
            "pred_res": pred_res,
            "gold_err": gold_err,
            "pred_err": pred_err,
            "exec_valid": exec_valid,
            "exec_correct": exec_correct,
            "jaccard": jac,
            "levenshtein": lev,
            "select_match": struct["select_match"],
            "where_match": struct["where_match"],
            "op_match": struct["op_match"],
        })

        # Optional verbose real-time logging
        if verbose and idx % 50 == 0:
            print(f"[{idx}/{sample_size}] exec_valid={exec_valid}, exec_correct={exec_correct}")

    df = pd.DataFrame(rows)

    if log_path:
        df.to_json(log_path, orient="records", lines=True)
        print(f"Saved evaluation log to {log_path}")

    elapsed = time.time() - start_time
    print(f"\nEvaluation completed in {elapsed/60:.2f} minutes.")

    return df

"""### Experiment Evaluation Result Helper Functions"""

def summarize_results(df):
    total = len(df)

    exec_valid = df["exec_valid"].mean()
    exec_correct = df["exec_correct"].mean()

    print("==== OVERALL METRICS ====")
    print(f"Examples evaluated: {total}")
    print(f"Execution Valid:    {exec_valid*100:.2f}%")
    print(f"Execution Correct:  {exec_correct*100:.2f}%")

    print("\n==== SEMANTIC SIMILARITY ====")
    print(f"Avg Jaccard:        {df['jaccard'].mean():.3f}")
    print(f"Avg Levenshtein:    {df['levenshtein'].mean():.3f}")
    print(f"Avg SELECT match:   {df['select_match'].mean():.3f}")
    print(f"Avg WHERE match:    {df['where_match'].mean():.3f}")
    print(f"Avg Operator match: {df['op_match'].mean():.3f}")

    return {
        "exec_valid": exec_valid,
        "exec_correct": exec_correct,
        "jaccard_mean": df["jaccard"].mean(),
        "lev_mean": df["levenshtein"].mean(),
    }

def plot_execution_accuracy(df, save_path=None):
    acc = {
        "Execution Valid": df["exec_valid"].mean(),
        "Execution Correct": df["exec_correct"].mean(),
    }
    plt.figure(figsize=(6,4))
    sns.barplot(x=list(acc.keys()), y=list(acc.values()))
    plt.ylim(0,1)
    plt.title("SQL Execution Metrics")
    plt.ylabel("Proportion")
    plt.grid(axis="y")
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

def plot_semantic_distributions(df, save_path=None):
    fig, ax = plt.subplots(1, 2, figsize=(12,5))

    sns.histplot(df["jaccard"], bins=30, ax=ax[0], kde=True)
    ax[0].set_title("Jaccard Similarity")

    sns.histplot(df["levenshtein"], bins=30, ax=ax[1], kde=True)
    ax[1].set_title("Levenshtein Similarity")

    if save_path:
        fig.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

def plot_error_breakdown(df, save_path=None):
    def classify(r):
        if r["exec_correct"]:
            return "Correct"
        if r["pred_err"] is not None:
            return "SQL Error"
        return "Wrong Result"

    df["err_type"] = df.apply(classify, axis=1)

    plt.figure(figsize=(6,4))
    sns.countplot(y="err_type", data=df, order=["Correct", "Wrong Result", "SQL Error"])
    plt.title("Error Type Breakdown")
    plt.grid(axis="x")
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

"""# Load Trained Model

### Load Baseline Model
"""

# Hugging Face model id
model_id = "google/gemma-3-4b-pt"  # pre-trained (not instruction-tuned)
# For tokenizer we use the instruction-tuned tokenizer
tokenizer_id = "google/gemma-3-4b-it"

# Select model class based on id
if model_id == "google/gemma-3-4b-pt":
    model_class = AutoModelForCausalLM
else:
    model_class = AutoModelForImageTextToText

# Choose dtype based on GPU capability
if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
    torch_dtype = torch.bfloat16
else:
    torch_dtype = torch.float16

# Define model init arguments
model_kwargs = dict(
    attn_implementation="sdpa", # Use "flash_attention_2" when running on Ampere or newer GPU
    torch_dtype=torch_dtype, # What torch dtype to use, defaults to auto
    device_map="auto", # Let torch decide how to load the model
)

model_kwargs["quantization_config"] = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_compute_dtype=model_kwargs['torch_dtype'],
    bnb_4bit_quant_storage=model_kwargs['torch_dtype'],
)


print("ðŸ”„ Loading model...")
base_model = model_class.from_pretrained(model_id, **model_kwargs)

print("ðŸ”„ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)

# Ensure tokenizer has EOS & PAD set correctly for generation & SFT
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left"

"""### Run Experiment on Baseline"""

eval_df = evaluate_model(
    base_model,
    sample_fraction=0.10,
    case_insensitive_sql=True,
    log_path=f"{TRAINED_MODEL_DIR}/eval_baseline_results.jsonl",
    verbose=True,
    seed=42
)

"""### Apply LoRA Adapters"""

ft_model = PeftModel.from_pretrained(base_model, TRAINED_MODEL_DIR)
ft_model.print_trainable_parameters()

"""### Test Model Output Anecdotally"""

examples = list(iter_split("test"))

for i in range(5):                        # get 5 examples
    ex, tbl = examples[i+1000]

    conn = build_sqlite_from_table(tbl)
    schema = make_schema_text(tbl)

    gold_sql = logical_to_sql(ex["sql"], tbl)
    pred_sql = generate_sql_from_llm(ex["question"], schema, ft_model)

    gold_res, ge = execute_sql(conn, gold_sql)

    pred_sql = apply_case_insensitive_matching(pred_sql)
    pred_res, pe = execute_sql(conn, pred_sql)

    print(f"\n### Example {i}")
    print("Q:", ex["question"])
    print("Gold:", gold_sql)
    print("Pred:", pred_sql)
    print("Gold result:", gold_res)
    print("Pred result:", pred_res if pe is None else f"ERROR: {pe}")

"""# Final Comparison Experiment

### Setup Experiment

### Run Experiment On Baseline Model

### Run Experiment on (3)? Trained Models
"""

eval_df = evaluate_model(
    ft_model,
    sample_fraction=0.10,
    case_insensitive_sql=True,
    log_path=f"{TRAINED_MODEL_DIR}/eval_results.jsonl",
    verbose=True,
    seed=42
)

summary = summarize_results(eval_df)
summary

plot_execution_accuracy(eval_df, save_path=f"{TRAINED_MODEL_DIR}/exec_accuracy.png")
plot_semantic_distributions(eval_df, save_path=f"{TRAINED_MODEL_DIR}/semantic_dist.png")
plot_error_breakdown(eval_df, save_path=f"{TRAINED_MODEL_DIR}/error_types.png")

def plot_execution_summary(df, save=False, save_path="execution_summary.png"):
    """
    df requires columns:
        - execution_valid (bool)
        - execution_correct (bool)
    """
    summary = pd.DataFrame({
        "Metric": ["Execution Valid", "Execution Correct"],
        "Proportion": [
            df["execution_valid"].mean(),
            df["execution_correct"].mean()
        ]
    })

    plt.figure(figsize=(8, 5))
    ax = sns.barplot(data=summary, x="Metric", y="Proportion", palette="Blues_d")

    ax.set_ylim(0, 1)
    ax.set_title("SQL Execution Performance Overview", fontsize=16, weight="bold")
    ax.set_ylabel("Proportion of Predictions")
    ax.set_xlabel("")

    plt.annotate(
        "Execution Valid = SQL executed without error\n"
        "Execution Correct = SQL result exactly matches expected output",
        xy=(0, -0.25), xycoords="axes fraction", fontsize=10
    )

    plt.tight_layout()

    if save:
        plt.savefig(save_path, bbox_inches="tight")
    plt.show()

def plot_similarity_distributions(df, save=False, save_path="similarity_scores.png"):
    """
    df requires columns:
        - jaccard
        - levenshtein
    """

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    sns.histplot(df["jaccard"], kde=True, ax=axes[0], bins=25, color="steelblue")
    axes[0].set_title("Jaccard Similarity Distribution", fontsize=15, weight="bold")
    axes[0].set_xlabel("Jaccard Score")
    axes[0].set_xlim(0, 1)

    sns.histplot(df["levenshtein"], kde=True, ax=axes[1], bins=25, color="seagreen")
    axes[1].set_title("Levenshtein Similarity Distribution", fontsize=15, weight="bold")
    axes[1].set_xlabel("Normalized Levenshtein Score")
    axes[1].set_xlim(0, 1)

    fig.suptitle("Semantic Similarity Between Gold & Predicted SQL", fontsize=17, weight="bold")

    plt.tight_layout()

    if save:
        plt.savefig(save_path, bbox_inches="tight")
    plt.show()

def plot_error_breakdown(df, save=False, save_path="error_breakdown.png"):
    """
    df requires a string column 'err_type' with categories like:
        - 'Correct'
        - 'Wrong Result'
        - 'SQL Error'
    """

    plt.figure(figsize=(9, 5))

    ax = sns.countplot(
        data=df,
        y="err_type",
        palette="coolwarm"
    )

    ax.set_title("SQL Prediction Error Breakdown", fontsize=16, weight="bold")
    ax.set_xlabel("Number of Test Cases")
    ax.set_ylabel("Error Category")

    # Add counts next to bars
    for container in ax.containers:
        ax.bar_label(container, fontsize=10, padding=5)

    plt.annotate(
        "â€¢ Correct = exact match result\n"
        "â€¢ Wrong Result = SQL valid but returned wrong answer\n"
        "â€¢ SQL Error = predicted SQL had a syntax or execution issue",
        xy=(1.02, 0.6), xycoords="axes fraction", fontsize=10
    )

    plt.tight_layout()

    if save:
        plt.savefig(save_path, bbox_inches="tight")
    plt.show()

def visualize_full_evaluation(df, save=False, prefix="evaluation_"):
    plot_execution_summary(
        df,
        save=save,
        save_path=f"{prefix}execution_summary.png"
    )

    plot_similarity_distributions(
        df,
        save=save,
        save_path=f"{prefix}similarity.png"
    )

    plot_error_breakdown(
        df,
        save=save,
        save_path=f"{prefix}error_breakdown.png"
    )

visualize_full_evaluation(eval_df, save=True, prefix="run1_")