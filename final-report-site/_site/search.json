[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "",
    "text": "A core competency of recent Large Language Models is an increasingly strong performance on structured translation tasks. The ability to convert text of natural language to computer understandable languages, like SQL, offers new opportunities to transform abstract requests for data into precise and repeatable queries. Text-to-SQL, describes the process of translating written text into SQL queries that can execute on running databases. This remains a challenging domain, especially for low parameter language models, due to the strict syntactic requirements of SQL, the need to ground queries in variable database schemas, and the brittleness of logical forms under small perturbations. Training full-scale large language models for such tasks is computationally expensive and practically infeasible for any non-enterprise application. Any reliance on commercially available model APIs likewise presents resource constraints. This project therefore investigates whether a parameter-efficient fine-tuning approach, using Low-Rank Adaptation (LoRA), can effectively specialize Google’s Gemma-3 4B model for use in basic text-to-SQL tasks. The project constructs a supervised fine-tuning pipeline using LoRA adapters of rank thirty-two applied across all major projection matrices, training only these adapters while the pre-trained backbone remains fixed and quantized in four-bit NF4 precision. A large text-to-SQL corpus, WikiSQL, provides general SQL generation competency, while evaluation occurs on a held out set of bifurcated WikiSQL tables to measure generalization.\n[PLACEHOLDER FOR BRIEF DESCRIPTION OF RESULTS HERE] The evaluation pipeline executes predicted SQL against SQLite databases derived from table metadata and compares both execution results and a suite of similarity metrics. The trained model exhibits substantial improvements across SQL validity, execution accuracy, lexical similarity, and structural SQL alignment. These results demonstrate the effectiveness of LoRA-based adaptation for SQL generation tasks in low-resource computational settings."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "",
    "text": "A core competency of recent Large Language Models is an increasingly strong performance on structured translation tasks. The ability to convert text of natural language to computer understandable languages, like SQL, offers new opportunities to transform abstract requests for data into precise and repeatable queries. Text-to-SQL, describes the process of translating written text into SQL queries that can execute on running databases. This remains a challenging domain, especially for low parameter language models, due to the strict syntactic requirements of SQL, the need to ground queries in variable database schemas, and the brittleness of logical forms under small perturbations. Training full-scale large language models for such tasks is computationally expensive and practically infeasible for any non-enterprise application. Any reliance on commercially available model APIs likewise presents resource constraints. This project therefore investigates whether a parameter-efficient fine-tuning approach, using Low-Rank Adaptation (LoRA), can effectively specialize Google’s Gemma-3 4B model for use in basic text-to-SQL tasks. The project constructs a supervised fine-tuning pipeline using LoRA adapters of rank thirty-two applied across all major projection matrices, training only these adapters while the pre-trained backbone remains fixed and quantized in four-bit NF4 precision. A large text-to-SQL corpus, WikiSQL, provides general SQL generation competency, while evaluation occurs on a held out set of bifurcated WikiSQL tables to measure generalization.\n[PLACEHOLDER FOR BRIEF DESCRIPTION OF RESULTS HERE] The evaluation pipeline executes predicted SQL against SQLite databases derived from table metadata and compares both execution results and a suite of similarity metrics. The trained model exhibits substantial improvements across SQL validity, execution accuracy, lexical similarity, and structural SQL alignment. These results demonstrate the effectiveness of LoRA-based adaptation for SQL generation tasks in low-resource computational settings."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "2 Introduction",
    "text": "2 Introduction\nText-to-SQL generation aims to translate natural language questions into executable database queries. While large language models increasingly exhibit strong zero-shot and few-shot reasoning capabilities, they struggle with the precision required for SQL generation. A single misplaced identifier or incorrect operator renders a query invalid. Moreover, SQL is always grounded in a specific schema: the model must understand which columns exist, how they relate to the question, and how to assemble a logically consistent query.\nFine-tuning the full weight matrices of an LLM for these behaviors requires significant computational resources. Parameter-efficient methods instead introduce small trainable modules into a frozen backbone. LoRA is among the most successful of these methods, projecting updates into low-dimensional subspaces and thereby reducing the number of trainable parameters by orders of magnitude. This project applies LoRA to Google’s Gemma-3 4B pretrained model with the objective of adapting the model toward structured SQL generation behaviors without updating its billions of parameters.\nThe project unfolds across three primary phases. First, a synthetic SQL dataset is used for supervised fine-tuning with LoRA, enabling the model to learn generic SQL patterning. Second, a standardized evaluation on WikiSQL is conducted using a custom SQLite execution pipeline. Third, model outputs are analyzed along semantic, syntactic, and execution dimensions. These phases jointly support the conclusion that LoRA-enabled fine-tuning can reliably transfer SQL generation capabilities from synthetic to real datasets."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "3 Literature Review",
    "text": "3 Literature Review\nThe design of this project draws from several key areas in the literature. Parameter-efficient fine-tuning methods, including LoRA, have shown that full-model updates are unnecessary for many downstream tasks. The LoRA formulation introduced by Hu et al. (2021) demonstrates that weight updates in large transformer models often lie near low-rank subspaces. A frozen weight matrix \\(W_0\\) is therefore augmented with a rank-\\(r\\) update expressed as\n\\[\nW = W_0 + \\Delta W \\qquad \\text{where} \\qquad \\Delta W = B A\n\\]\nwhere \\(A \\in \\mathbb{R}^{k \\times r}\\) and \\(B \\in \\mathbb{R}^{r \\times d}\\) are trainable matrices with \\(r \\ll \\min(d, k)\\). This formulation considerably reduces the number of trainable parameters while enabling expressive adaptation.\nDettmers et al. (2023) introduced QLoRA, which combines LoRA updates with four-bit quantization of the frozen base model. QLoRA demonstrated that powerful models can be fine-tuned on single-GPU hardware, laying the foundation for the training strategy used in this project. By quantizing the Gemma-3 4B model into NF4 format and training only LoRA adapters, this project follows the practical recommendations of QLoRA to maximize efficiency and stability.\nThe foundational computational structures of transformer models, including self-attention, residual pathways, and feed-forward projections, follow directly from the course materials in DSAN 5800. Larson (2025a) reviews sequence modeling fundamentals and motivates the use of attention mechanisms. Larson (2025b) expands this foundation into full transformer architectures, describing multi-head self-attention, positional encoding, and the stacking of transformer blocks. These theoretical components directly inform the deeper architectural review provided in this report.\nFinally, the WikiSQL dataset has played a central role in text-to-SQL research since its introduction by Zhong et al. It is notable for its constrained SQL grammar and the large variety of tables, which makes it suitable for evaluating generalization across schema-specific tasks. Existing research demonstrates that models must integrate schema information and natural language semantics to perform well on WikiSQL. This project adopts the standard execution-based evaluation criterion, aligning with prior work.\nTogether, these strands of research motivate the methodological design of this project: use LoRA to specialize a mid-sized LLM for SQL generation, leverage four-bit quantization to control memory costs, and evaluate rigorously using execution correctness."
  },
  {
    "objectID": "index.html#dataset",
    "href": "index.html#dataset",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "4 Dataset",
    "text": "4 Dataset\nThis project relies exclusively on the WikiSQL dataset for both supervised fine-tuning and evaluation. The approach taken here uses only real WikiSQL examples so that the fine-tuned model learns directly from the human-generated question–SQL pairs and table schemas. This constraint has the large benefit that it simplifies the experimental setting and ensures that evaluation reflects performance on the same structural distribution seen during training.\n\n4.1 Dataset Characteristics\nWikiSQL is well-suited for studying foundational text-to-SQL behavior for three primary reasons. First, it features a simple but still realistic SQL grammar, where each query operates over exactly one table, includes at most a single aggregation operator, and consists of a conjunction of comparatively shallow filtering conditions. This design isolates the core translation problem—mapping natural language onto SQL relational operators—while avoiding confounds such as multi-table joins, nested subqueries, or complex predicate structure. Second, WikiSQL provides unusually high schema and task diversity: it contains over 24,000 unique tables, each with different column names, column orders, and data distributions. Because every example is grounded in its own table, the model cannot rely on memorized column labels or SQL templates, and must instead learn generalizable alignment between linguistic references (e.g., “what is the population”) and arbitrary schema fields. This property makes WikiSQL a powerful testbed for evaluating schema-conditioned generalization in LLMs. Third, the dataset is mature, publicly validated, and widely adopted in the semantic parsing community. With more than 80,000 natural-language questions paired with structured SQL programs, WikiSQL offers a sufficiently large corpus for supervised fine-tuning, along with clear execution-based evaluation metrics that allow unambiguous correctness checking.\nA critical aspect of WikiSQL, what allows for supervised fine tuning, is that each example includes not only a question and table schema, but also a canonical SQL program that expresses the correct or expected answer for each natural language question. This canonical program, often referred to as the gold query, serves as the ground-truth target during training and evaluation. The presence of these structured gold queries enables both supervised fine-tuning and execution-based evaluation where the predicted query generated by an LLM and the gold query can be run against the same table to verify whether they return equivalent answers. The inclusion of gold queries is one of the primary reasons WikiSQL remains such a widely used benchmark.\nIn keeping with best practices for machine learning, the training, validation, and test sets are partitioned such that there is no overlap of tables between them and each unique table occurs in only one split. This ensures that model evaluation reflects genuine generalization to unseen schemas, rather than memorization of patterns or data from previously observed tables. A summary of the dataset sizes used in this project is shown below:\n\n\n\nSplit\nRows\nUsed For\n\n\n\n\nTrain\n56,355\nTraining set\n\n\nDev\n8,421\nValidation & monitoring\n\n\nTest\n15,878\nTest Set\n\n\n\n\n\nThese counts correspond to the examples that remained after filtering malformed SQL programs, invalid schemas, or tables that could not be serialized cleanly. Each surviving row is represented using four structured features\n\nA question in ordinary English.\nThe table schema, specifying all column names and data types.\nThe target SQL query, represented as both a logical tuple and executable SQL text.\nExample table rows for reconstructing the corresponding SQLite table.\n\nExample WikiSQL Data Item\n\nQuestion: Tell me what the notes are for South Australia\n\nTable Headers:\n- State/territory\n- Text/background colour\n- Format\n- Current slogan\n- Current series\n- Notes\n\nExample Rows:\n| State/territory             | Text/background colour | Format      | Current slogan                       | Current series | Notes                                 |\n|-----------------------------|-----------------------|-------------|--------------------------------------|---------------|---------------------------------------|\n| Australian Capital Territory| blue/white            | Yaa·nna     | ACT · CELEBRATION OF A CENTURY 2013  | YIL·00A       | Slogan screenprinted on plate         |\n| New South Wales             | black/yellow          | aa·nn·aa    | NEW SOUTH WALES                      | BX·99·HI      | No slogan on current series           |\n| New South Wales             | black/white           | aaa·nna     | NSW                                  | CPX·12A       | Optional white slimline series        |\n| Northern Territory          | ochre/white           | Ca·nn·aa    | NT · OUTBACK AUSTRALIA               | CB·06·ZZ      | New series began in June 2011         |\n| Queensland                  | maroon/white          | nnn·aaa     | QUEENSLAND · SUNSHINE STATE          | 999·TLG       | Slogan embossed on plate              |\n| South Australia             | black/white           | Snnn·aaa    | SOUTH AUSTRALIA                      | S000·AZD      | No slogan on current series           |\n| Victoria                    | blue/white            | aaa·nnn     | VICTORIA - THE PLACE TO BE           | ZZZ·562       | Current series will be exhausted this year |\n\nLogical SQL:\n  sel  = 5\n  agg  = 0\n  conds = [[3, 0, \"SOUTH AUSTRALIA\"]]\n\nExecutable SQL:\n  SELECT \"Notes\"\n  FROM data\n  WHERE \"Current slogan\" = 'SOUTH AUSTRALIA'\n\n\n\n4.2 Preprocessing and Prompt Construction\nTo convert WikiSQL examples into training-ready sequences, each example is reformatted into a prompt–completion pair using a strict, instruction-driven structure. This design aligns with the constraints of a causal decoder-only model such as Gemma, which generates output exclusively through next-token prediction. By placing the SQL query in the completion rather than embedding it within the prompt, the supervision signal directly mirrors the model’s inference-time behavior.\nThe prompt itself is organized into clearly delineated sections - &lt;INSTRUCTIONS&gt;, &lt;SCHEMA&gt;, &lt;QUESTION&gt;, and &lt;SQL_Query&gt; - each represented by explicit opening and closing tags that appear verbatim in the input text. These tags function as unambiguous structural boundaries, ensuring that the model can reliably distinguish between natural-language instructions, schema metadata, question content, and the region where SQL generation begins. Such explicit segmentation has been shown to stabilize text-to-SQL training by reducing format drift, preventing the model from mixing explanation and code, and framing SQL prediction as a discrete subtask rather than a free-form generation problem. The resulting prompt format is therefore both human-interpretable and optimized for predictable model behavior.\nImportantly, in the training pipeline all row level table data and the logical SQL encodings (sel/agg/conds) are intentionally excluded from the LLM’s input. Only the schema representation and the natural language question are included in the prompt, and only the executable SQL string appears in the completion. This design mirrors practical deployment settings, where exposing full table contents would be infeasible due to size, privacy, or API constraints. It also forces the model to infer column relevance, aggregation type, and filtering conditions purely from the question–schema alignment rather than memorizing example values. The exclusion of table rows prevents shortcut learning, such as exploiting statistical correlations between specific cell values and question patterns. By contrast, the serialized schema gives the model exactly the minimal structural information required to produce syntactically valid and semantically grounded SQL queries. The strict tag structure and constrained input-output interface jointly act as a curriculum and, along with the consistent instructions field, they work to guide the model toward producing clean, well-formed SQL while minimizing opportunities for hallucination or format drift.\nThe schema is serialized into the following uniform text structure:\nPROMPT FIELD:\n\n&lt;INSTRUCTIONS&gt;\nYou are a precise text-to-SQL generator. Using the known schema of the sql \ndatabase you must output only a valid SQL query and nothing else.\n&lt;/INSTRUCTIONS&gt;\n\n&lt;SCHEMA&gt;\n- State/territory (TEXT)\n- Text/background colour (TEXT)\n- Format (TEXT)\n- Current slogan (TEXT)\n- Current series (TEXT)\n- Notes (TEXT)\n&lt;/SCHEMA&gt;\n\n&lt;QUESTION&gt;\nTell me what the notes are for South Australia\n&lt;/QUESTION&gt;\n\n&lt;SQL_Query&gt;\n\nCOMPLETION FIELD:\n\nSELECT \"Notes\" FROM data \nWHERE \"Current slogan\" = 'SOUTH AUSTRALIA'\n&lt;/SQL_Query&gt;\n\n\n\n4.3 SQLite Reconstruction and Executable SQL\nFor evaluation, each WikiSQL table is materialized as a lightweight, in-memory SQLite database so that both gold and model-generated queries can be executed under a consistent, realistic SQL semantics. SQLite is a natural choice in this setting because it supports the core subset of SQL operators used in WikiSQL (selection, aggregation, and simple conjunctions), imposes strict syntactic requirements that expose malformed queries, and can be constructed directly from the table headers and rows provided in the dataset. Concretely, each table object is first converted into a Pandas DataFrame using the original column headers and row values.\nThis DataFrame is then written into an ephemeral SQLite database under a fixed table name (data) as shown:\nimport sqlite3\n\n# df is the DataFrame representation of the WikiSQL table\nconn = sqlite3.connect(\":memory:\")  # create an in-memory SQLite database\ndf.to_sql(\"data\", conn, index=False, if_exists=\"replace\")\nThe logical SQL encoding provided by WikiSQL (sel, agg, conds) is mapped into an executable SQL string using the logical_to_sql() helper, which expands the selected column index, aggregation operator, and condition tuples into a syntactically valid SELECT ... FROM data WHERE ... statement.\nAt evaluation time, both the gold SQL and the model’s predicted SQL are executed against the same in-memory database using a shared execute_sql() wrapper:\ndef execute_sql(conn, sql):\n    try:\n        cursor = conn.execute(sql)\n        return cursor.fetchall()\n    except Exception as e:\n        return f\"Error: {e}\"\nThis design serves multiple purposes: it enforces that the model produce not just syntactically valid SQL but also semantically executable queries; it decouples the modeling code from any particular storage backend; and it enables execution-based metrics (e.g., whether the predicted query returns exactly the same result set as the gold query) that directly reflect end-to-end task success."
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "5 Methods",
    "text": "5 Methods\n\n5.1 Gemma-3 Model Architecture\nGemma-3 4B is a mid-sized transformer-based language model designed by Google with efficiency, adaptability, and fine-tuning stability as primary objectives. At approximately four billion parameters, it occupies a practical middle ground: large enough to support high-quality text generation and structured reasoning tasks, yet compact enough to be fine-tuned on a single modern GPU using parameter-efficient methods such as LoRA. The model follows the general decoder-only transformer paradigm common to contemporary large language models, but introduces several refinements—including hybrid attention patterns, large vocabulary capacity, and optimized projection structures—to improve performance on long-context, instruction-following, and code-generation tasks.\nAt a high level, Gemma-3 4B consists of 34 transformer blocks, each operating over a hidden dimensionality of 2560 units. The architecture uses rotary positional embeddings to encode word order and maintain relative positional invariances, which improves generalization across sequence lengths. Attention is implemented via multi-head self-attention with 8 heads, and select layers employ sliding-window attention to reduce computational cost on long sequences. These local attention blocks are interspersed with globally attentive layers to periodically refresh holistic contextual representations, a strategy that allows Gemma-3 to scale to long contexts without incurring the quadratic cost of full attention at every layer.\nA summary of the most relevant architectural hyperparameters is presented below:\n\n\n\n\n\n\n\nFeature\nValue / Description\n\n\n\n\nTotal Trainable Parameters\n1,342,504,960 (~23%)\n\n\nTotal Non-trainable Parameters\n5,708,161,392\n\n\nNumber of Layers\n34 Transformer blocks\n\n\nHidden Size\n2560\n\n\nAttention Heads\n8 (4 key/value heads)\n\n\nMLP Intermediate Size\n10,240\n\n\nActivation Function\nGELU-Tanh\n\n\nVocabulary Size\n262,208 tokens\n\n\nPositional Encoding\nRoPE with linear scaling (×8), effective context window: 131k tokens\n\n\nAttention Type\nSliding-window (1024 tokens) + global full attention every 6 blocks\n\n\n\n\n\nEach transformer block in Gemma-3 follows the established pattern of attention → feed-forward → residual connection, but with careful engineering to improve numerical stability and training efficiency:\n\nMulti-Head Self-Attention (MHSA).\nThe model computes learned projections for queries, keys, and values using matrices \\(W_Q\\), \\(W_K\\), and \\(W_V\\). These are grouped into multiple heads, which allows the model to attend to different types of dependencies in parallel. Attention is computed via the scaled dot-product formulation:\n\\[\n\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{Q K^\\top}{\\sqrt{d_k}} \\right) V\n\\]\nThe outputs of all heads are concatenated and passed through an output projection \\(W_O\\) before being added to the residual pathway.\nFeed-Forward Network (FFN).\nFollowing attention, each block applies a feed-forward MLP of the form\n\\[\n\\mathrm{FFN}(x) = W_{\\mathrm{down}}\\, f\\big(W_{\\mathrm{up}}\\, x\\big)\n\\]\nwhere \\(f\\) is a smooth nonlinearity such as SiLU. The upward projection expands the dimensionality, allowing for richer intermediate representations, while the downward projection compresses the result back to the hidden dimension.\nNormalization and Residual Structure.\nEach sublayer includes normalization layers to regulate activation magnitudes and improve gradient flow. The residual pathways serve as shortcuts that mitigate vanishing-gradient issues and stabilize both pretraining and fine-tuning.\n\n\n\n5.2 Low-Rank Adaptation (LoRA) Configuration\nLoRA is a parameter-efficient adaptation method that tunes large language models by inserting small, trainable matrices—called adapters—into selected linear projections within a frozen base transformer. For a given base weight matrix \\(W_0\\), LoRA augments the weights with a low-rank update as follows:\n\\[\nW = W_0 + \\frac{\\alpha}{r} A B\n\\]\nwhere \\(A\\) and \\(B\\) are trainable matrices of size \\((d, r)\\) and \\((r, k)\\), respectively (\\(r\\) denoting the low-rank dimension), and \\(\\alpha\\) is a scaling factor to stabilize and scale the update. Only the small \\(A\\) and \\(B\\) matrices are modified during training; the full weight matrix \\(W_0\\) remains fixed. This setup dramatically reduces the number of tunable parameters, as the count now grows linearly with \\(r\\) instead of with the size of \\(W_0\\).\nIn our configuration, the LoRA adapters use rank \\(r = 32\\) and scaling factor \\(\\alpha = 16\\), providing a strong balance between flexibility and stability on moderate hardware. Adapters are inserted into the most key linear projections: the query, key, value, and output weights in attention, as well as the gate, up, and down projections in the feed-forward sublayers. This placement enables the model to efficiently adapt its mechanisms for parsing schema and generating SQL from natural language.\nImportantly, LoRA provides memory efficiency not just during training, but also when saving and deploying models. Because only the adapter (LoRA) weights are updated, these small matrices—rather than the entire set of base model weights—can be saved after fine-tuning. This means that storage and sharing of task-specific models becomes highly efficient: just the adapter file needs to be retained, and later it can be “attached” or merged with any compatible base model for easy reuse or deployment, without duplicating the large pretrained model files.\nOur broader training approach follows the QLoRA framework: the base Gemma weights are held fixed and stored in quantized (NF4) precision for substantial GPU memory savings, while the smaller LoRA adapters are kept at higher precision for effective optimization. This decoupling lets us train and deploy strong models even on limited hardware, and further maximizes storage and sharing efficiency by siloing the small, learnable adapter parameters from the large, frozen base model.\nThe Colab training environment configures LoRA using the following settings\npeft_config = LoraConfig(\n    r=32,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    modules_to_save=[\"lm_head\", \"embed_tokens\"]\n)\nThis configuration directs LoRA to focus its adaptation capacity on the projections most critical for SQL reasoning. It also preserves the token embedding and final language modeling head in full precision which helps maintain stable output distributions during training. The result is an efficient and effective fine tuning mechanism that improves task specific behavior without modifying the billions of pretrained parameters in the Gemma model.\n\n\n5.3 Training Plan\nThe goal of the training pipeline is to adapt the Gemma 3 model, with roughly four billion parameters, to the text-to-SQL task using LoRA adapters while keeping the pretrained backbone frozen and quantized. The method follows a causal language modeling objective, which means that the model is trained to predict the next token in a sequence that contains both the prompt and the gold SQL completion. Because SQL prediction in deployment is also a continuation-based process, the training objective and inference behavior remain aligned, reducing the risk of format drift and encouraging the model to internalize SQL generation as a structured next-token prediction problem. Only the LoRA parameters are updated during training while the transformer backbone stays fixed in 4-bit NF4 precision. This separation provides a stable optimization surface and keeps the computational and memory demands well within the limits of a single GPU environment.\nThe WikiSQL dataset, once preprocessed, is fed into the model as a series of prompt–completion pairs. Each prompt contains an instruction block, a serialized table schema, and the user question, all wrapped in explicit XML-like tags that allow the tokenizer and model to reliably partition the semantic components of each example. The completion field then contains the corresponding SQL query ending with a closing tag. These examples are loaded into Hugging Face Datasets objects, which provide efficient memory mapping and automatic integration with the tokenization pipeline. During training, the SFTTrainer prepares each example by concatenating the prompt and SQL completion into a single sequence that reflects the exact text the model will see during inference. The tokenizer then applies Gemma’s byte-pair encoding rules to convert text into token IDs, and the trainer assembles these token sequences into batches. Because packing=True is enabled, multiple short examples are combined into the same 512-token window, reducing padding waste and ensuring that GPU memory is used efficiently.\nOnce tokenized, each sequence is copied into a parallel label sequence that is shifted by one position. This means that for token position t, the model attempts to predict token t+1. The resulting cross-entropy loss is computed only over the completion region of each sequence, so the model is not penalized for regenerating instructions or schema information. This selective masking ensures that the optimization pressure focuses on SQL formation rather than reproducing prompt structure. The causal objective therefore becomes a direct mechanism for teaching the model how to form valid SQL: each incorrect token prediction increases the loss, and the LoRA adapters update in a direction that improves the syntactic and semantic correctness of future predictions.\nThe mechanics of loss reduction follow the standard structure of a decoder-only transformer. For every token position, Gemma produces a probability distribution over the entire vocabulary. The training signal compares this distribution to the true next token through a cross-entropy objective. When the predicted distribution assigns too little mass to the correct SQL token, the associated gradient flows backward through the LoRA adapter matrices and updates them to improve future predictions. The frozen 4-bit backbone never changes, but it continues to supply rich contextual and semantic embeddings. The LoRA matrices essentially “bend” these pretrained features toward SQL behavior without disturbing the foundational linguistic knowledge encoded in Gemma. Gradient accumulation is employed to expand the effective batch size: although the nominal batch size per device is one, gradients from four successive forward–backward passes are aggregated before the optimizer performs a weight update. This method stabilizes training while avoiding memory overflows.\nThe training configuration defined through SFTConfig governs all aspects of optimization and resource management. The 512-token sequence length reflects the short nature of WikiSQL examples and avoids unnecessary cost. Packed sequences improve throughput, which is particularly important for quantized models whose performance often depends on efficient batching. Three epochs provide adequate exposure to the training distribution without encouraging memorization of table-specific patterns, especially because WikiSQL partitions tables across splits to prevent leakage. The learning rate is intentionally conservative, following QLoRA recommendations, because adapter tuning on a quantized backbone can become unstable at higher rates. A warmup period ensures that the optimizer does not immediately apply large updates, which helps prevent divergence in the early stages of training. Mixed precision, either FP16 or BF16 depending on hardware capabilities, reduces memory usage and speeds up computation. The use of gradient checkpointing further lowers memory pressure by recomputing intermediate activations during the backward pass. The constant learning rate scheduler maintains a steady optimization environment once warmup is complete. Each of these settings contributes to an efficient and stable fine-tuning procedure suitable for limited hardware.\nDuring training, the SFTTrainer logs several metrics that provide insight into learning dynamics. The primary signal is the training loss, which shows how effectively the model is predicting SQL tokens as training progresses. Additional logs include the active learning rate, the number of tokens processed per second, and internal trainer statistics.\nThe training pipeline relies on an integrated stack of Python libraries. Hugging Face Transformers provides the Gemma architecture along with quantization hooks that allow NF4 loading through BitsAndBytes. The TRL library supplies the SFTTrainer, which automates dataset packing, label masking, forward passes, and loss computation for causal LM objectives. The PEFT library manages LoRA initialization, weight injection, and adapter merging. Hugging Face Datasets provides the backbone for efficient preprocessing and dataset streaming. SQLite and Pandas support evaluation, enabling real SQL execution through a lightweight, in-memory database. TensorBoard records scalar metrics that allow visual inspection of the loss curves throughout training. This entire stack is orchestrated inside a Google Colab environment, with persistent storage managed through Google Drive and experiment metadata captured through timestamped directories.\nFrom end to end, training proceeds through a clear sequence of steps. WikiSQL examples are loaded, validated, and converted into prompt–completion structures. These examples are tokenized, packed, and fed into the model during supervised tuning. The Gemma backbone remains fixed while LoRA adapters gradually learn task-specific behavior. The trainer monitors loss and writes checkpoints at the end of each epoch. After fine-tuning is complete, the adapter weights and tokenizer are saved, and the model is evaluated through SQL execution on reconstructed SQLite tables.\n\n\n5.4 Text to SQL Experimental Setup\nThe evaluation procedure is designed to compare two models under identical conditions. The first model is the unmodified Gemma 3 4B pretrained checkpoint, which serves as the baseline. The second is the fine-tuned LoRA-augmented Gemma 3 4B model produced by the training pipeline described earlier. Both models are evaluated using the same prompt generation process, the same SQL cleaning and extraction logic, and the same SQLite execution environment. This design ensures that measured performance differences derive from learned SQL generation behavior rather than from changes in preprocessing or evaluation tooling.\nEvaluation begins by converting each WikiSQL test example into the same structured prompt format used during supervised training. The prompt consists of an instruction block, a serialized representation of the table schema, and the natural-language question. These components are wrapped in explicit XML-like tags, which allow the model to identify the point where SQL generation begins. The model receives no information about table rows during inference; it must infer the correct aggregation, selection, and filtering operations entirely from the schema and question. The prompt is then fed into the model, which generates a continuation consisting of a predicted SQL query. The query is extracted and cleaned through a standardized parsing step that identifies the first valid SQL-like region inside the  block and removes any trailing explanation or stray text the model may have produced. This procedure, applied uniformly to both models, isolates the predicted SQL in a consistent and deterministic manner.\nThe predicted SQL is evaluated in several ways. First, the query is executed against a reconstructed SQLite database created from the table rows provided in the WikiSQL test set. Because each table in WikiSQL includes fully specified column names and row data, it is straightforward to materialize a corresponding SQLite table inside an in-memory database and then execute both the gold query and the predicted query within that environment. Execution results are compared directly. If the predicted SQL runs without a syntax error and returns the same result rows as the gold query, it is marked as correct. If the query is executable but returns an incorrect result set, it is counted as a wrong-result case. If the SQL engine raises a syntax or runtime error, the prediction is categorized as invalid. This execution-based evaluation is the most stringent test available because it verifies both syntactic correctness and semantic alignment with the true relational operation described in the question.\nBeyond execution correctness, the system performs a suite of similarity measurements. These include Jaccard token similarity, normalized Levenshtein similarity, and structural alignment metrics that compare selected columns, referenced columns in the WHERE clause, and operator usage. These structural scores are important because SQL queries can differ superficially while still expressing the same computation, and in other cases, they can appear similar while differing in a subtle but meaningful way. Text-based metrics such as BLEU and ROUGE are computed as well, along with exact-match comparisons and token-level F1 scores. The combination of execution and similarity metrics provides a multidimensional picture of the model’s predictive behavior and helps identify whether errors arise from structural misunderstandings, lexical drift, or outright semantic failures.\nInference with Gemma 3 4B is not instantaneous. Each query requires prompt construction, tokenization, autoregressive generation, SQL cleaning, schema materialization, and SQLite execution. The average end-to-end time for a single evaluation example is approximately fifteen seconds in the Colab environment used for this project. Running the full WikiSQL test set, which contains over fifteen thousand examples, would therefore require several hours of continuous inference, far exceeding the time available under practical constraints. To address this constraint while preserving statistical integrity, the evaluation operates on a random ten percent sample of the full test set. A reproducible random seed selects this subsample, allowing the experiment to run within an acceptable time window while maintaining representative coverage of the underlying data distribution. Each sampled example goes through the full evaluation sequence, and the aggregated results constitute the reported performance metrics for both the baseline and trained models.\nThe final evaluation step involves computing summary statistics over all predicted queries. These summaries include the proportion of syntactically valid SQL queries, the proportion that execute correctly, averaged similarity metrics, and aggregate BLEU, ROUGE, EM, and F1 values. Row-level logs are saved in JSONL format, preserving each question, predicted SQL, gold SQL, execution result, and error type, while a separate summary file records the global metrics that form the basis of the quantitative results in the report. All evaluations are performed twice, once for the baseline model and once for the LoRA-fine-tuned model, enabling a direct and controlled comparison of the effect of LoRA adaptation on SQL generation behavior."
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "6 Results",
    "text": "6 Results\n\n6.1 Training Results\n\n\n\nFig. 1:\n\n\n\n\n\nFig. 1:\n\n\n\n\n\nFig. 1:\n\n\n\n\n6.2 Text to SQL Evaluation Results\n\n6.2.1 Baseline\n\n\n\nFig. 2:\n\n\n\n\n\nFig. 3:\n\n\n\n\n\nFig. 4:\n\n\n\n\n\nFig. 4:\n\n\n\n\n6.2.2 Trained LoRA Model (Rank 32, Alpha 16)\n\n\n\nFig. 2:\n\n\n\n\n\nFig. 3:\n\n\n\n\n\nFig. 4:\n\n\n\n\n\nFig. 4:\n\n\n\n\n6.2.3 Example Prompt Comparison\nTable Preview (first rows)\n\n\n\n\n\n\n\n\n\n\n\nTotal#\nSeries#\nTitle\nWriter\nDirector\nOriginal air date\n\n\n\n\n14\n1\n” Sister Hood ”\nDominic Minghella\nCiaran Donnelly\n6October2007, 7.30–8.15pm\n\n\n15\n2\n” The Booby and the Beast ”\nSimon Ashford\nCiaran Donnelly\n13October2007, 7.30–8.15pm\n\n\n16\n3\n” Childhood ”\nJason Sutton\nCiaran Donnelly\n20October2007, 7.15–8.00pm\n\n\n17\n4\n” The Angel of Death ”\nJulian Unthank\nMatthew Evans\n27October2007, 7.15–8.00pm\n\n\n18\n5\n” Ducking and Diving ”\nDebbie Oates\nMatthew Evans\n3November2007, 7.15–8.00pm\n\n\n\nSchema Used - Total# (TEXT) - Series# (TEXT) - Title (TEXT) - Writer (TEXT) - Director (TEXT) - Original air date (TEXT)\nQuestion:\nWhat’s the title of the episode that Rob Heyland wrote?\nGold SQL Query\nSELECT \"Title\" FROM data WHERE \"Writer\" = 'Rob Heyland'\n\nBaseline Model Generated SQL\nSELECT Title FROM Total WHERE Writer = 'Rob Heyland'\nTrained Model Generated SQL\nSELECT \"Title\" FROM data WHERE \"Writer\" = 'Rob Heyland'\nSimilarity Metrics\n\n\n\nMetric\nBaseline Value\nTrained LoRa Value\n\n\n\n\nExact Match\n0\n1\n\n\nJaccard\n0.5\n1\n\n\nLevenshtein\n0.873\n1\n\n\nToken F1\n0.667\n1\n\n\n\n\nExecution Summary\n\n\n\nItem\nValue\n\n\n\n\nBaseline Result\nERROR: no such table: Total\n\n\nTrained LoRa Result\n[(‘” For England…! “’,)]\n\n\nGold Result\n[(‘” For England…! “’,)]"
  },
  {
    "objectID": "index.html#discussion",
    "href": "index.html#discussion",
    "title": "Using LoRA to Fine Tune Gemma Models for Text to SQL",
    "section": "7 Discussion",
    "text": "7 Discussion"
  }
]